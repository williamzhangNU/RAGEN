defaults:
  - base
  - evaluate_api_llm

output_path: results/spatial/passive/claude-3.7/e2a.json

eval_model_type: api # api or vllm
model_path: Qwen/Qwen2.5-3B-Instruct
model_config:
  model_name: claude-3.7 # gpt-4o
  max_concurrency: 16

actor_rollout_ref:
  rollout:
    max_model_len: 10000
    gpu_memory_utilization: 0.8

agent_proxy:
  max_turn: 1 # 1 for passive, 8 for active
  action_sep: "||"
  max_actions_per_turn: 1 # how many actions can be output at most in a single turn

es_manager:
  val:
    env_groups: 32
    group_size: 1 # should be set to 1 because when val temperature is set to 0 and group size > 1, there will be repetitive prompts which leads to same trajectory.
    env_configs:
      tags: ["PassiveE2A"]
      n_groups: [32] # TODO: If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation
